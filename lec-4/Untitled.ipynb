{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ramblings: Generativos vs. Discriminativos\n",
    "\n",
    "## ¿Por qué LDA es generativo si tiene discriminante en su nombre?\n",
    "\n",
    "> _de los creadores de \"Y si son tan comunistas, ¿por qué tienen iPhone?_\n",
    "\n",
    "* Cuento corto: Esta división entre generativo/discriminativo se formalizó a principios de los 80s. Se transformó en la nomenclatura dominante.\n",
    "\n",
    "* LDA (o Discriminante Lineal de Fisher) se remonta a 1936. Por un alcance de nombre ahora todos estamos confundidos.\n",
    "\n",
    "## Definición de Generativo y Discriminativo (Muchas Gracias Josué!)\n",
    "\n",
    "> Chicos, así lo entiendo yo:\n",
    "> 1. Discriminativo: Aprendo la probabilidad condicional $\\textsf{Pr}(y \\vert x)$, es decir, la probabilidad de la clase $y$ dadas mis variables $x\\in\\mathbf{X}$. O sea, *discrimino* entre las clases.\n",
    "> 2. Generativo: Aprendo la probabilidad conjunta $\\textsf{Pr}(x, y)$, que me permite calcular la verosimilitud o likelihood $\\textsf{Pr}(x \\vert y)$. Es decir, genero un modelo para $y$, y luego, usando el modelo, puedo decir la probabilidad de que las variables $x$ hayan sido *generadas* por mi modelo. Ese justamente es $\\textsf{Pr}(x \\vert y)$, la probabilidad de las variables dado el modelo. Usando el teorema de bayes puedo luego saber la probabilidad a posteriori $\\textsf{Pr}(y \\vert x)$. Finalmente como tengo un modelo para la clase y puedo eventualmente *generar* variables _x_ que tengan una alta probabilidad de pertenecer a la clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo funciona el teorema de Bayes dentro de los algoritmos generativos?\n",
    "\n",
    "##### Setup de un modelo generativo: \n",
    "\n",
    "1. Evaluamos la probabilidad conjunta para todos los atributos $\\textsf{Pr}(x,y) \\quad \\forall x \\in \\mathbf{X}^{\\mathbb{R}}$.\n",
    "- Buscamos resolver $\\textsf{Pr}(y\\vert \\mathbf{X})$. Para ello implementamos el teorema de Bayes.\n",
    "- Un aspecto a considerar es que la probabilidad conjunta se establece: $\\textsf{Pr}(x, y) = \\textsf{Pr}(x\\vert y) \\textsf{Pr}(y)$. Esto es la verosimilitud por la información a priori.\n",
    "- Fórmula clásica:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\underset{\\textsf{A posteriori}}{\\textsf{Pr}(y \\vert \\mathbf{X})}\\propto \\frac{\\underset{\\textsf{Verosimilitud (Pr conjunta)}}{\\textsf{Pr}(x\\vert y)}\\underset{\\textsf{A priori}}{\\textsf{Pr}(y)}}{\\underset{\\textsf{Evidencia}}{\\sum_{i=1}^{y}\\textsf{Pr}(x\\vert y)\\textsf{Pr}(y)}}\n",
    "$$\n",
    "\n",
    "\n",
    "![](img/mcelreath-bayes.png)\n",
    "> _Fuente: Kruschke, J. 2014. Doing Bayesian Data Analysis: a Course with R, Stan and JAGS._ (Sí, el de los perritos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mi modelo vale yuyo ¿Qué hago?\n",
    "\n",
    "1. En la medida de lo posible __extraer más datos de entrenamiento__. Es asintóticamente eficiente <3.\n",
    "- Ante la imposibilidad de aumentar la cantidad de registros en entrenamiento, consideren estos aspectos:\n",
    "    - Feature Engineering  $\\leadsto$ Mis recodificaciones tienen sentido o no?\n",
    "    - Feature selection $\\leadsto$ Estoy agregando/eliminando ruido a mi modelo?\n",
    "    - Hiperparámetros $\\leadsto$ Y si soy yo el que se equivoca?\n",
    "    - Over/Undersampling $\\leadsto$ Hagámonos los locos y __asumamos que vamos a igualar el problema de clases desbalanceadas__.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Si todo lo demás falla, siempre pueden partir una vida nueva en Miami protegidos por el FBI\n",
    "\n",
    "![](img/jadue.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varianza interna y externa\n",
    "\n",
    "* Tomemos una regresión lineal como punto de partida:\n",
    "\n",
    "$$\n",
    "y = \\beta_{0} + \\beta_{1}\\textsf{X}_{j}+\\varepsilon_{ij}\n",
    "$$\n",
    "\n",
    "donde $i$ hace referencia a todos los registros en una matriz, y $j$ hace referencia a __algún grupo__ $j \\in \\mathcal{J}$\n",
    "\n",
    "* El punto de conflicto es el siguiente: __El error se puede decomponer en partes fijas y aleatorias__.\n",
    "* Esto da pie para hablar de _MUUUUCHAS COSAS_, sobretodo en Modelación Multinivel (bioestadística) y Efectos fijos y aleatorios (econometría / inferencia causal). No vamos a hacer eso. ~~Pero si quieren saber más, pueden leer Data Analysis using Regression and Multilevel/Hierarchical models de Gelman y Hill <3~~.\n",
    "\n",
    "##### Decomposición del Error\n",
    "\n",
    "![](img/anova.png)\n",
    "\n",
    "$$\n",
    "\\varepsilon_{ij} = \\underset{\\textsf{Parte interna}}{\\nu_{i}} + \\underset{\\textsf{Parte externa}}{\\delta_{j}}\n",
    "$$\n",
    "\n",
    "- Parte interna $\\leadsto$ Varianza _dentro_ de un grupo específico (respecto a la media grupal).\n",
    "- Parte externa $\\leadsto$ Varianza _entre_ los grupos específicos (respecto a la media muestral general).\n",
    "\n",
    "Podría entregar formalidades en ecuaciones, pero pucha que lata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Over(Under) Sampling\n",
    "\n",
    "* Métodos clásicos $\\leadsto$ Repetir registros.\n",
    "    - Problema: Si los datos son malos, estaremos replicando estas fallas.\n",
    "    \n",
    "* Solución (A grandes rasgos!) $\\leadsto$ En función a un subconjunto de datos correspondiente a la clase minoritaria, entrenamos algún método como KMeans o KNearestNeighbors para generar representaciones sintéticas de los datos __en el espacio de atributos de la clase específica__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
