{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ramblings: Tercera forma de validación, Latent Dirichlet Allocation y GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Train, Test y Hold Out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ">  The Three-Way Holdout Method for Hyperparameter Tuning (Raschka, 2018)\n",
    "\n",
    "\n",
    "\n",
    "##### ¿Qué conocemos?\n",
    "\n",
    "![](img/two-stage-cv.png)\n",
    "\n",
    "* El proceso de búsqueda de hiperparámetros se puede considerar como una tarea de __meta-optimización__.\n",
    "* Es bueno separar el proceso de optimización de la función objetivo del proceso de meta optimización. \n",
    "* El problema es que si incorporamos nuestra norma de validación (Test Set) dentro de la grilla de optimización, estaremos ajustando los hiperparámetros __en función al desempeño del testing__, lo que se puede considerar como trampa :(\n",
    "\n",
    "##### ¿Qué deseamos saber?\n",
    "\n",
    "![](img/three-stage-cv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un par de comentarios\n",
    "\n",
    "\n",
    "Si trabajamos con `GridSearchCV`, tan solo haciendo la división en dos muestras es suficiente, incorporando training a nuestra clase y reservando train como hold out\n",
    "Si tenemos un archivo de testing externo, tal vez no sea necesario hacer división (?)\n",
    "\n",
    "* `sklearn.model_selection.GridSearchCV().cv_results_` presenta una infinidad de cosas!\n",
    "    - `split*` Indican la métrica específica en cada validación cruzada y combinación de hiperparámetros.\n",
    "    - `*_time` Indican el tiempo de ejecución.\n",
    "    - Por lo general trabajamos con `mean_test_score`, `mean_train_score`.\n",
    "    \n",
    "* `sklearn.model_selection.GridSearchCV().best_estimator_` devuelve un modelo listo para entrenar con la mejor combinación de hiperparámetros!\n",
    "* `sklearn.model_selection.GridSearchCV().best_score_` devuelve el desempeño promedio del modelo en el testing interno. Si es clasificación devuelve Accuracy, si es regresión devuelve MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "LDA busca inferir tópicos mediante el siguiente proceso: Cada documento dentro de un corpus se compone de una __mezcla de tópicos__ que se encuentran alojados __a nivel de corpus__. La estructura de tópicos es latente $\\leadsto$ sólo observamos los documentos y las palabras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `n_components`: Cantidad de tópicos a inferir.\n",
    "* `learning_decay`: Tasa de aprendizaje en la función de pérdida. Cuando se utiliza con `online`, la optimización se hace con SGD (Stochastic Gradient Descent)\n",
    "* `learning_method`:\n",
    "* Perplejidad: Busca aproximar el número óptimo de tópicos a inferir. Técnicamente evalua qué tan bien predice una muestra específica. En función a un número $K$ de tópicos, define la distribución teórica de palabras reopresentada por los tópicos y la compara con la ocurrencia empírica de palabras en tópicos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlap\n",
    "\n",
    "https://mpatacchiola.github.io/blog/2016/11/12/the-simplest-classifier-histogram-intersection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers en Latent Dirichlet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import minimal_helpers as helpers\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = glob.glob(\"./data/*.csv\")\n",
    "append_csv = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_files:\n",
    "    append_csv.append(pd.read_csv(i, index_col=None, header=0).drop(columns='Unnamed: 0'))  \n",
    "    \n",
    "df = pd.concat(append_csv)\n",
    "df.columns = ['artist', 'genre', 'song', 'lyrics'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop('genre')\n",
    "n = .33\n",
    "seed = 11238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hold_in, X_hold_out, y_hold_in, y_hold_out = train_test_split(X, y, test_size=n,\n",
    "                                                               random_state=seed)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_hold_in, y_hold_in, test_size=n,random_state=seed)\n",
    "\n",
    "hold_in_counter = CountVectorizer(stop_words='english', min_df=.10, max_features=5000)\n",
    "hold_in_transformed_features = hold_in_counter.fit_transform(X_hold_in['lyrics'])\n",
    "\n",
    "hold_out_counter = CountVectorizer(stop_words='english', max_df=.1, max_features=5000)\n",
    "hold_out_transformed_features = hold_out_counter.fit_transform(X_hold_out['lyrics'])\n",
    "\n",
    "train_counter = CountVectorizer(stop_words='english', max_df=.1, max_features=5000)\n",
    "train_transformed_feats = train_counter.fit_transform(X_train['lyrics'])\n",
    "\n",
    "test_counter = CountVectorizer(stop_words='english', max_df=.1, max_features=5000)\n",
    "test_transformed_feats = test_counter.fit_transform(X_test['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_params = {'n_components': [5, 10, 15], 'learning_decay': [0.7, 0.5]}\n",
    "cv_lda_model = GridSearchCV(\n",
    "    LatentDirichletAllocation(learning_method='online'),\n",
    "    param_grid = search_params,\n",
    "    n_jobs=-1).fit(hold_in_transformed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfil de perplejidad del modelo (\"entropía existente en el modelo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista vacía para preservar perplejidad\n",
    "store_hold_in_perplexity = []\n",
    "# lista vacía para guardar combinación de hiperparametros\n",
    "store_combination = []\n",
    "\n",
    "for i in [5, 10, 15]:\n",
    "    for j in [0.5, 0.7]:\n",
    "        # instanciamos el objeto\n",
    "        tmp = LatentDirichletAllocation(n_components = i,\n",
    "                                       learning_decay = j,\n",
    "                                       learning_method = 'batch')\n",
    "        # generamos el fit en los feats transformados\n",
    "        tmp.fit(hold_in_transformed_features)\n",
    "        # adjuntamos perplejidad\n",
    "        store_hold_in_perplexity.append(tmp.perplexity(hold_in_transformed_features))\n",
    "        # adjuntamos mejor combinación de hiperparametros\n",
    "        store_combination.append(f\"K={i}, eta={j}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(store_hold_in_perplexity, '-o')\n",
    "plt.xticks(range(len(store_combination)), store_combination, rotation=45);\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_lda_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lda = cv_lda_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de principales palabras asociadas en tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.identify_words_associated(best_lda, # insertamos el mejor modelo\n",
    "                                  hold_in_counter, # insertamos el contador de ocurrencias\n",
    "                                  20) # cantidad de palabras por tópico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = helpers.identify_most_likely_topic(best_lda, #mejor modelo\n",
    "                                              hold_in_counter, # contador de ocurrencias\n",
    "                                              hold_in_transformed_features, # texto tokenizado\n",
    "                                              X_hold_in) #datos a insertar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(infer_df.loc[:, 'Tópico 1':'Tópico 15'].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "helpers.report_artist_topic(infer_df, 'Eminem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
