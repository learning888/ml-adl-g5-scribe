{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ramblings: Naive Bayes, Algoritmos Generativos, Batch Feed y trabajando con texto\n",
    "\n",
    "### Sobre Algoritmos Generativos vs. Discriminativos\n",
    "\n",
    "> Caveat: Intuición de Ng, Andrew. 2013. CS299: Machine Learning. Department of Computer Science. Stanford University\n",
    "\n",
    "\n",
    "Si tenemos dos tipos de animales, Elefantes ($y=1$) y Perros ($y=0$), buscamos predecir a una nueva observación en base a una serie de atributos $x\\in\\mathbf{X}$. Ante este problema, existen dos escuelas de pensamiento:\n",
    "\n",
    "- __Algoritmos generativos__: Generamos un resumen o representación sobre cuáles son las principales características de un Elefante y Perro, en función a la comunalidad de atributos __para cada clase__. De esta manera estamos observando la probabilidad de clase __en función a la probabilidad conjunta de los atributos__. Con nuestros resumenes de cada clase, evaluamos si una observación presenta similitudes en algunos de los dos grupos y asignamos la clase en base al grupo con mayor similitud.\n",
    "\n",
    "- __Algoritmos discrimminativos__: Buscamos generar una recta divisoria (llamada frontera de decisión) que busque separar la observaciones Elefantes y Perro. Posteriormente para clasificar una nueva observación, evaluamos en que lado de la recta se sitúa. \n",
    "\n",
    "#### ¿Y qué es mejor?\n",
    "\n",
    "> Caveat: La intuición proviene de Jordan y Ng (2001), Shalev-Shwartz y Ben-David (2014)\n",
    "\n",
    "Resulta que la principal diferencia entre un modelo generativo y uno discriminativo radica en la cantidad de casos:\n",
    "\n",
    "- Un modelo generativo tiene una tasa de error asintótico más alto (condicional a la cantidad de observaciones en nuestro conjunto de entrenamiento).\n",
    "- En los modelos generativos existe el paso intermedio donde evaluamos la probabilidad conjunta de los datos y la clase $\\textsf{Pr}(\\mathbf{X}, y)$. El problema de este paso es que puede capturar ruido antes de evaluar la estrategia de maximización a posteriori.\n",
    "- La máxima sugiere implementar modelos discriminativos por sobre generativos, dado que evitamos este error de forma innecesaria.\n",
    "- Un punto a favor de los generativos es que cuando la cantidad de datos de entrenamiento es baja, el modelo generativo alcanza la cota asintótica del error mucho más rápido que un modelo discriminativo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas en problemas de clasificación\n",
    "\n",
    "- __Precision__: Razón entre predicciones positivas correctas en el total de las predicciones positivas totales.\n",
    "- __Recall__: Razón entre predicciones positivas correctas sobre el total de predicciones correctamente identificadas.\n",
    "\n",
    "\n",
    "Existe un trade off que depende mucho del problema de clasificación al cual nos enfrentamos. Podemos estar en situaciones donde deseamos maximizar la precisión (evitar que una observación sea clasificada incorrectamente) y  no tomar tanto en cuenta el recall (podemos tolerar un grado menor de observaciones con falsos negativos)\n",
    "\n",
    "Aún así, el óptimo es encontrar precision y recall altos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Feed\n",
    "\n",
    "* ¿Cómo realizar la ingesta de una cantidad substancial de datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_tribe_called_quest_scrape.csv     michael_jackson_scrape.csv\r\n",
      "anthrax_scrape.csv                  mobb_deep_scrape.csv\r\n",
      "black_star_scrape.csv               modest_mouse_scrape.csv\r\n",
      "bob_dylan_scrape.csv                mos_def_scrape.csv\r\n",
      "britney_spears_scrape.csv           necrophagist_scrape.csv\r\n",
      "bruce_springsteen_scrape.csv        nickelback_scrape.csv\r\n",
      "cannibal_corpse_scrape.csv          nicki_minaj_scrape.csv\r\n",
      "carly_rae_jepsen_scrape.csv         nwa_scrape.csv\r\n",
      "de_la_soul_scrape.csv               oasis_scrape.csv\r\n",
      "deicide_scrape.csv                  opeth_scrape.csv\r\n",
      "dr._dre_scrape.csv                  pink_floyd_scrape.csv\r\n",
      "dua_lipa_scrape.csv                 public_enemy_scrape.csv\r\n",
      "eminem_scrape.csv                   queen_scrape.csv\r\n",
      "faith_no_more_scrape.csv            radiohead_scrape.csv\r\n",
      "ghostface_killah_scrape.csv         raekwon_scrape.csv\r\n",
      "gorgoroth_scrape.csv                rage_against_the_machine_scrape.csv\r\n",
      "immortal_scrape.csv                 red_hot_chili_peppers_scrape.csv\r\n",
      "incubus_scrape.csv                  rush_scrape.csv\r\n",
      "iron_maiden_scrape.csv              sam_smith_scrape.csv\r\n",
      "kanye_west_scrape.csv               sia_scrape.csv\r\n",
      "kendrick_lamar_scrape.csv           slayer_scrape.csv\r\n",
      "killer_mike_scrape.csv              spice_girls_scrape.csv\r\n",
      "kiss_scrape.csv                     system_of_a_down_scrape.csv\r\n",
      "led_zeppelin_scrape.csv             the_beatles_scrape.csv\r\n",
      "lorde_scrape.csv                    the_clash_scrape.csv\r\n",
      "mayhem_scrape.csv                   the_doors_scrape.csv\r\n",
      "megadeth_scrape.csv                 the_smashing_pumpkins_scrape.csv\r\n",
      "meshuggah_scrape.csv                the_smiths_scrape.csv\r\n",
      "metallica_scrape.csv                tool_scrape.csv\r\n",
      "method_man_scrape.csv               vital_remains_scrape.csv\r\n",
      "mf_doom_scrape.csv                  weezer_scrape.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls dump/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos una lista con todos los csv existentes\n",
    "file_list = glob.glob(os.getcwd() + '/dump/*.csv')\n",
    "# lista vacía\n",
    "\n",
    "append_csv = []\n",
    "# para cada csv\n",
    "for f in file_list:\n",
    "    # abrir en pandas y concatenar\n",
    "    append_csv.append(pd.read_csv(f, index_col=None, header=0).drop(columns='Unnamed: 0'))\n",
    "# concatenar la lista de dataframes\n",
    "\n",
    "\n",
    "df_lyrics = pd.concat(append_csv)\n",
    "df_lyrics.columns = ['Artist', 'Genre', 'Song', 'Lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windows amigos - not tested though\n",
    "\n",
    "a = [name for name in os.listdir(\"./dump/\") if name.endswith(\".csv\")]\n",
    "append_csv = []\n",
    "# para cada csv\n",
    "for f in file_list:\n",
    "    # abrir en pandas y concatenar\n",
    "    append_csv.append(pd.read_csv(f, index_col=None, header=0).drop(columns='Unnamed: 0'))\n",
    "# concatenar la lista de dataframes\n",
    "df_lyrics = pd.concat(append_csv)\n",
    "df_lyrics.columns = ['Artist', 'Genre', 'Song', 'Lyrics']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9489, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajo con texto en `sklearn`\n",
    "\n",
    "* Si tenemos una serie $N$ de documentos con $d$ frases y $w$ palabras, un algoritmo clásico de ML no podrá trabajar de forma bruta con la secuencia de valores alfanumérica. Razones:\n",
    "    - No existe una unidad vectorial clara.\n",
    "    - Los documentos puede tener un tamaño variante en frases y palabras.\n",
    "    \n",
    "* __Solución__: Nuestro objetivo va a ser transformar una serie de documentos en una matriz (dispersa). Pasos:\n",
    "    - __Tokenizar__: Separaremos cada palabra existente en todos los documentos (también llamado corpus) y le asignaremos un identificador entero. Algunas decisiones de tokenización: Espacios en blanco, signos de puntuación.\n",
    "    - __Contar__: la frecuencia de cada token __en cada documento__.\n",
    "    - __Ponderar y normalizar__ aquellos tokens con una menor o mayor ocurrencia.\n",
    "    \n",
    "* Estructura de datos: \n",
    "    - cada palabra (o token) se puede considerar como un atributo específico.\n",
    "    - cada documento se compone de distintas ocurrencias de tokens.\n",
    "    \n",
    "Este proceso se conoce como _Bag of Words_ $\\leadsto$ Cada documento se describe como una combinación de ocurrencia de palabras, asumiendo __ignorabilidad completa__ a la posición relativa de la palabra en la oración.    \n",
    "    \n",
    "    \n",
    "#### CountVectorizer\n",
    "\n",
    "Implementa un conteo discreto de cada palabra en el _Bag-of-Words_. Existen otras variantes como `FeatureHasher`, `TfidfVectorizer` que buscan agilizar el proceso y ponderar por la ocurrencia general en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
